{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd03ed8ad36289d305c36e0f62c4336657b59989454aba0ee7799c35f3bccb4e530",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies. Install if not previously installed\n",
    "j = 1\n",
    "while j<=1:\n",
    "  try:\n",
    "    import fasttext \n",
    "    import requests, os \n",
    "    import pandas as pd \n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    from pycountry import languages #for formating\n",
    "    import re, string #for preprocessing\n",
    "    j=2\n",
    "  except Exception as e:\n",
    "    !pip install fasttext\n",
    "    !pip install pycountry\n",
    "    !pip install tqdm\n",
    "    j+=1"
   ]
  },
  {
   "source": [
    "### Functions\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download pretrained model if not downloaded before.\n",
    "def download_fastText_model(path, overwrite_model = False) -> None:\n",
    "    '''Download pretrained fasttext language detection model to specified path.\n",
    "       REMEMBER 2 PAPERS TO REFERENECE FOR MODEL, URL: https://fasttext.cc/docs/en/language-identification.html'''    \n",
    "    try:\n",
    "        assert not (os.path.exists('data/lid.176.bin') and overwrite_model == False)\n",
    "        r = requests.get('https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin')\n",
    "        open(path,'wb').write(r.content)\n",
    "    except AssertionError:\n",
    "        print('Model exists and is not overwritten')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocesseing \n",
    "def preprocess_string(x:'input string',\n",
    "                     remove_URLs = True,\n",
    "                     remove_tagged_user = True,\n",
    "                     lowercase = True,\n",
    "                     remove_numbers = True,\n",
    "                     remove_punctuation = True,\n",
    "                     remove_extra_white_space = True,\n",
    "                     remove_stopwords = True, stopwords:'list of stopwords' = None,\n",
    "                     remove_listofwords = True, listofwords:'list of other words to be removed' = None\n",
    "                     ) -> 'preprocessed string':\n",
    "    \n",
    "    '''This function converts values to strings and apply a set of preprocessing steps. '''\n",
    "\n",
    "    cleaned = str(x)\n",
    "    \n",
    "    #0 Remove URLs\n",
    "    if remove_URLs == True:\n",
    "       cleaned = re.sub('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?','',cleaned)\n",
    "\n",
    "    #1 Remove all tagged users, i.e. words starting with the ‘@’ character.\n",
    "    if remove_tagged_user == True:\n",
    "        cleaned = re.sub('@\\w*','',cleaned)\n",
    "    \n",
    "    if remove_listofwords == True and listofwords != None:\n",
    "        cleaned = ' '.join([word for word in cleaned.split() if word not in listofwords])\n",
    "\n",
    "    #2 Lowercase all tweet text.\n",
    "    if lowercase == True:\n",
    "        cleaned = cleaned.lower()\n",
    "\n",
    "    #3 Remove numbers.\n",
    "    if remove_numbers == True:\n",
    "        cleaned = re.sub('[0-9]','',cleaned)\n",
    "\n",
    "    #4 Remove punctuation. \n",
    "    if remove_punctuation == True: \n",
    "        cleaned = cleaned.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #5 Remove extra whitespaces.\n",
    "    if remove_punctuation == True:\n",
    "        cleaned = re.sub(\"\\s+\",\" \",cleaned)\n",
    "    \n",
    "    #6 Remove stopwords.\n",
    "    if remove_stopwords == True and stopwords != None:\n",
    "        cleaned = ' '.join([word for word in cleaned.split() if word not in stopwords])\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_language(x, model:'pretrained fastText model' ) -> '(estimate, estimate_confidence)':\n",
    "    '''Finds the most relevant language using a pretrained fastTrack api, which \n",
    "    can be downloaded with the download_fastText_model function.'''\n",
    "\n",
    "    #predict from pretrained fastText-model:\n",
    "    pred = model.predict(x)\n",
    "    \n",
    "    #convert results to language name\n",
    "    language_iso = re.sub('__label__','',pred[0][0])\n",
    "    try:\n",
    "        language_name = languages.get(alpha_2 = language_iso).name\n",
    "    except Exception as e:\n",
    "        language_name = None \n",
    "    #prediction acc.\n",
    "    acc = pred[1][0]\n",
    "\n",
    "    return (language_name, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isretweet(x:'string'):\n",
    "    try:\n",
    "        assert re.match('^RT', string = x)\n",
    "        return True\n",
    "    except AssertionError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_sample_to_excel(df, sample_count,path, sample_names = None, sample_size_unique = 0, sample_size_collective = 0):\n",
    "    ''' Create samples and export them to folder '''\n",
    "    unique_sample = [df.sample(n = sample_size_unique) for x in range(sample_count)]\n",
    "    collective_sample = df.sample(n = sample_size_collective)\n",
    "    combined_sample = [x.append(collective_tweets) for x in unique_sample]\n",
    "    \n",
    "    if sample_names == None: sample_names = [str(x) for x in range(sample_count)]\n",
    "    \n",
    "    [x.to_excel(f'{path}/{sample_names[y]}.xlsx') for y, x in enumerate(combined_sample)]\n"
   ]
  },
  {
   "source": [
    "### Script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tweets\n",
    "tweets = pd.read_excel('data/resistance_tweets.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\espen\\anaconda3\\lib\\site-packages\\tqdm\\std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 240423/240423 [00:21<00:00, 11054.58it/s]\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Model exists and is not overwritten\n"
     ]
    }
   ],
   "source": [
    "#Clean tweets\n",
    "tqdm.pandas()\n",
    "tweets['tweet_cleaned'] = tweets['tweet'].progress_apply(lambda x: preprocess_string(x,listofwords = ['RT']))\n",
    "\n",
    "#import model\n",
    "PRETRAINED_MODEL_PATH = 'data/lid.176.bin'\n",
    "download_fastText_model(download_fastText_model, overwrite_model = False) #downloads model if not downloaded before\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH) #create model object to be parsed in the find language function\n"
   ]
  },
  {
   "source": [
    "#### Find language of tweets on a per tweet basis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 240423/240423 [00:14<00:00, 16185.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Find relevant language\n",
    "tweets['language'] = tweets['tweet_cleaned'].progress_apply(lambda x: find_language(x, model = model)) \n",
    "\n",
    "#format DataFrame \n",
    "new_col_list = ['language_estimate','language_estimate_acc']\n",
    "for n,col in enumerate(new_col_list):\n",
    "    tweets[col] = tweets['language'].apply(lambda x: x[n])\n",
    "tweets = tweets.drop('language',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    tweet  language_estimate_acc\n",
       "language_estimate                               \n",
       "English            172786               0.824318\n",
       "Dutch               38065               0.938708\n",
       "German              21623               0.963159\n",
       "French               2748               0.300501\n",
       "Japanese             2160               0.967022\n",
       "Finnish              1622               0.954531\n",
       "Spanish               155               0.461261\n",
       "Russian               144               0.488025\n",
       "Portuguese            100               0.495158\n",
       "Serbian                90               0.473764"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>language_estimate_acc</th>\n    </tr>\n    <tr>\n      <th>language_estimate</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>English</th>\n      <td>172786</td>\n      <td>0.824318</td>\n    </tr>\n    <tr>\n      <th>Dutch</th>\n      <td>38065</td>\n      <td>0.938708</td>\n    </tr>\n    <tr>\n      <th>German</th>\n      <td>21623</td>\n      <td>0.963159</td>\n    </tr>\n    <tr>\n      <th>French</th>\n      <td>2748</td>\n      <td>0.300501</td>\n    </tr>\n    <tr>\n      <th>Japanese</th>\n      <td>2160</td>\n      <td>0.967022</td>\n    </tr>\n    <tr>\n      <th>Finnish</th>\n      <td>1622</td>\n      <td>0.954531</td>\n    </tr>\n    <tr>\n      <th>Spanish</th>\n      <td>155</td>\n      <td>0.461261</td>\n    </tr>\n    <tr>\n      <th>Russian</th>\n      <td>144</td>\n      <td>0.488025</td>\n    </tr>\n    <tr>\n      <th>Portuguese</th>\n      <td>100</td>\n      <td>0.495158</td>\n    </tr>\n    <tr>\n      <th>Serbian</th>\n      <td>90</td>\n      <td>0.473764</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "language_table = tweets.groupby('language_estimate').agg({'tweet':'count', 'language_estimate_acc':'mean'})\n",
    "language_table.sort_values(by='tweet',ascending = False).head(10)\n",
    "#tweets.loc[tweets['language_estimate']=='Dutch',:]"
   ]
  },
  {
   "source": [
    "#### Language detection on a per author basis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 240423/240423 [00:00<00:00, 249133.92it/s]\n",
      "100%|██████████| 230/230 [00:02<00:00, 83.59it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 115118.13it/s]\n",
      "100%|██████████| 230/230 [00:00<00:00, 234034.43it/s]\n"
     ]
    }
   ],
   "source": [
    "#create retweet column\n",
    "tweets['retweet'] = tweets['tweet'].progress_apply(lambda x: isretweet(str(x)))\n",
    "#Concatenate tweets by author (no retweets)\n",
    "tweets['tweet_cleaned_no_rt'] = np.where(tweets['retweet']==True, '', tweets['tweet_cleaned'] )\n",
    "\n",
    "tweets['AllTweetsByAuthor'] = tweets.groupby(['username'])['tweet_cleaned_no_rt'].transform(lambda x: ' '.join(x))\n",
    "tweets_all = tweets.drop_duplicates(['username']).copy()\n",
    "\n",
    "#Find relevant language\n",
    "tweets_all['language'] = tweets_all['AllTweetsByAuthor'].progress_apply(lambda x: find_language(str(x), model = model)) \n",
    "\n",
    "#format DataFrame \n",
    "new_col_list = ['author_language_estimate','author_language_estimate_acc']\n",
    "for n,col in enumerate(new_col_list):\n",
    "    tweets_all[col] = tweets_all['language'].progress_apply(lambda x: x[n])\n",
    "tweets_all = tweets_all.drop('language',axis=1)\n",
    "tweets = pd.merge(left = tweets, right = tweets_all.loc[:,['username','author_language_estimate','author_language_estimate_acc' ]], left_on = 'username', right_on = 'username' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop concatenated tweet column\n",
    "#tweets = tweets.drop('AllTweetsByAuthor', axis = 1)\n",
    "\n",
    "#Create english tweet dummy\n",
    "tweets['EnglishTweet'] = pd.get_dummies(\n",
    "                    (tweets['author_language_estimate']=='English') & \n",
    "                    ((tweets['language_estimate'] == 'English')  |  (tweets['language_estimate_acc'] <0.5)),\n",
    "                    drop_first = True)\n",
    "\n",
    "#save\n",
    "tweets.to_csv('data/preprocessed_tweets_with_language.csv')\n"
   ]
  },
  {
   "source": [
    "## Create sample of tweets to code manually"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Preleminary open code sample: 20 unique tweets per person and 20 tweets that everyone get."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_sample = tweets.loc[(df['retweet']==False),:]\n",
    "\n",
    "twitter_sample_to_excel(df_to_sample, 4,'data', sample_size_unique = 20, sample_size_collective = 20)\n"
   ]
  },
  {
   "source": [
    "Create sample with only english tweets by english classified authors. A tweet is english if the accuracy score of the FastText api is either english or if it is another english (by an english author) with an accuracy of less than x pct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_sample = tweets.loc[(tweets['EnglishTweet'] == 1) & (df['retweet']==False),:]\r\n",
    "\r\n",
    "twitter_sample_to_excel(df_to_sample, 4,'data', sample_size_unique = 0, sample_size_collective = 20)\r\n",
    "\r\n",
    "\r\n",
    "df_to_sample"
   ]
  },
  {
   "source": [
    "#### Produce 500 tweets for manual coding after testing above"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_sample = tweets.loc[(tweets['EnglishTweet'] == 1) & (df['retweet']==False),['tweet_id','tweet']]\n",
    "twitter_sample_to_excel(df_to_sample, 4, 'data', \n",
    "                        sample_names = ['E_500_tweets','AM_500_tweets','AJ_500_tweets', 'K_500_tweets'],\n",
    "                        sample_size_unique = 500,\n",
    "                        sample_size_collective = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mtwitter_sample_to_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_size_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msample_size_collective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m Create samples and export them to folder \n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\espen\\documents\\sds\\dm_and_asds2_exam_project\\<ipython-input-91-b8695177e4ab>\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "?twitter_sample_to_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}